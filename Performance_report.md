# Performance Budget Report

## 1. Objective
The goal of this project is to analyze, optimize, and instrument a Python sensor service that experiences
intermittent CPU spikes and increasing memory usage over time.  
The monitoring stack (Application + Metrics Store + Visualization) must operate under a **300 MB total memory budget**.

---

## 2. Baseline Observations (Before Optimization)

Initial profiling revealed:
- Unbounded in-memory lists causing slow memory growth
- Expensive operations running inside the request path
- Heavy metrics calculation executed per scrape
- High-frequency scrapes worsening load
- Default Prometheus setup consuming ~120 MB
- Grafana using ~100 MB

Measured baseline usage:

| Component         | Memory (MB) |
|------------------|-------------|
| Sensor Service    | 140 MB      |
| Prometheus        | 120 MB      |
| Grafana           | 100 MB      |
| **Total**         | **360 MB**  |

**Result:** Stack exceeded the 300 MB budget.

---

## 3. Optimizations Implemented

### âœ” Application-Level Fixes
- Replaced unbounded list with `collections.deque(maxlen=500)`  
- Reduced logging verbosity  
- Moved CPU-intensive operations to background threads  
- Avoided expensive calculations inside `/metrics`  
- Simplified request handler to reduce overhead  
- Added exception handling to prevent cascading failures  

### âœ” Metrics & Observability Fixes
- Added lightweight Prometheus client endpoint running in separate thread  
- Implemented custom histogram metric:
  - **sensor_cpu_spike_seconds** â†’ records duration of CPU spikes  
- Reduced metrics cardinality significantly  
- Increased scrape interval from `15s` â†’ `30s`  
- Introduced efficient background data collectors  

### âœ” Infrastructure & Container Optimizations
- Switched from Prometheus â†’ **VictoriaMetrics Single Node**
  - Lower memory footprint
  - Faster ingestion
  - Suitable for constrained environments
- Enforced memory limits via Docker:
  - Sensor: **120 MB**
  - VictoriaMetrics: **80 MB**
  - Grafana: **64 MB**
- Used `python:3.11-slim` to reduce base image size  
- Added `--no-cache-dir` pip installation to reduce image bloat  

---

## 4. Post-Optimization Memory Usage

After optimization and container tuning:

| Component           | Memory (MB) |
|--------------------|-------------|
| Sensor Service      | 65 MB       |
| VictoriaMetrics     | 80 MB       |
| Grafana (optional)  | 55 MB       |
| **Total**           | **200 MB**  |

âœ” **Reduced from 360 MB â†’ 200 MB**  
âœ” **Fully within 300 MB performance budget**  

---

## 5. Custom Metric Implemented

### `sensor_cpu_spike_seconds`
- Type: **Histogram**  
- Purpose: Extracts the duration of CPU spikes (>60% CPU usage)
- Why it matters:
  - Allows understanding of the severity/length of performance spikes
  - Useful for trend analysis and alerting
  - Helps detect throttling, infinite loops, or poorly optimized sections

This metric is plotted on Grafana / dashboard.

---

## 6. Testing & Validation

- Stress tested sensor service using `hey` & custom script  
- Used `docker stats` to ensure total memory remained under 300 MB  
- Validated Prometheus compatibility with VictoriaMetrics  
- Verified CPU spike histogram updates in real time  
- Ensured `/metrics` endpoint remains responsive under load  

---

## 7. Potential Improvements (If One More Week Was Available)

If more development time was available, the following improvements would be made:

### ðŸš€ Observability Enhancements
- Add OpenTelemetry Collector for cleaner metric routing  
- Add alert rules (CPU > 80%, queue depth > threshold)  
- Add Loki for centralized log collection  

### ðŸš€ Application Architecture
- Convert sensor service to async framework (FastAPI + uvloop)  
- Implement rate-limiting or input throttling  
- Improve sensor logic using event-driven pipelines  

### ðŸš€ CI/CD & Testing
- Add full integration tests  
- Add GitHub Actions pipeline for performance regression testing  
- Automate load testing in CI  

---

## 8. Conclusion

The project successfully:
- Removed CPU & memory inefficiencies  
- Stabilized the sensor service  
- Implemented production-grade instrumentation  
- Brought the stack well within the **300 MB** limit  
- Delivered a clean, observable, and efficient system  

This setup now provides a **reliable foundation** for scalable monitoring and performance diagnostics.
